./context_filter.rs:            let goal_symbols: Vec<&str> = get_words(&scraped.context.focused_goal())
./context_filter.rs:            // goal arguments, they are actually fresh variables
./context_filter.rs:                goal_symbols
./context_filter_parser.lalrpop:    "goal-args" => ContextFilterAST::GoalArgs,
./features.rs:                &remove_paths_from_goal(scraped.context.focused_goal()),
./features.rs:                goal_head_feature(tmap, &remove_paths_from_goal(scraped.context.focused_goal())),
./features.rs:                // (std::cmp::min(get_symbols(&datum.context.focused_goal()).len(), 100) as f64) / 100.0,
./features.rs:    goal: &String,
./features.rs:        &remove_paths_from_goal(goal),
./features.rs:        goal_head_feature(tmap, &remove_paths_from_goal(goal)),
./features.rs:        best_score, // (std::cmp::min(get_symbols(&goal).len(), 100) as f64) / 100.0,
./features.rs:pub fn goal_head_feature(tmap: &TokenMap, goal: &str) -> i64 {
./features.rs:    match goal.split_whitespace().next() {
./features.rs:        Some(first_token) => match tmap.goal_token_to_index.get(first_token) {
./features.rs:    goal_token_to_index: HashMap<String, usize>,
./features.rs:        let index_to_goal_token = index_common(
./features.rs:                .flat_map(|scraped| remove_paths_from_goal(scraped.context.focused_goal()).split_whitespace().next())
./features.rs:            goal_token_to_index: flip_vec(index_to_goal_token),
./features.rs:            (self.goal_token_to_index.len() + 2) as i64,
./features.rs:            self.goal_token_to_index.clone(),
./features.rs:            goal_token_to_index: dicts.1,
./features.rs:        let mut index_to_goal_token = vec![""; self.goal_token_to_index.len()];
./features.rs:        for (goal_token, index) in self.goal_token_to_index.iter() {
./features.rs:                index < &self.goal_token_to_index.len(),
./features.rs:                "index is {}, but there are only {} goal tokens",
./features.rs:                self.goal_token_to_index.len()
./features.rs:            index_to_goal_token[*index] = goal_token;
./features.rs:        data.insert("goal_tokens", index_to_goal_token);
./features.rs:        let (goal_tokens, tactics, hyp_tokens) = match json_data {
./features.rs:                match (vals["goal_tokens"].clone(),
./features.rs:            goal_token_to_index: flip_vec(goal_tokens),
./features.rs:    goal: &String,
./features.rs:    let truncated_goal: String = remove_paths_from_goal(goal).chars().take(128).collect();
./features.rs:            gestalt_ratio(remove_paths_from_goal(goal), &get_hyp_type(hyp).chars().take(128).collect::<String>())
./features.rs:    goal: &String,
./features.rs:        let score = gestalt_ratio(remove_paths_from_goal(goal), get_hyp_type(hyp));
./features.rs:fn remove_paths_from_goal(goal: &str) -> String {
./features.rs:    println!("{}", goal);
./features.rs:    let mut updated_goal: String = "".to_string();
./features.rs:    let words: Vec<&str> = goal.split(" ").collect();
./features.rs:        updated_goal = updated_goal + " " + split[0];
./features.rs:    println!("{}", updated_goal);
./features.rs:    updated_goal
./lib.rs:use models::goal_enc_evaluator::*;
./lib.rs:        goal: String,
./lib.rs:            goal,
./lib.rs:        goal: &str,
./lib.rs:        decode_fpa_result_rs(args, metadata, hyps, goal, tac_idx, arg_idx)
./lib.rs:        goal: String,
./lib.rs:        get_premise_features_rs(args, metadata, goal, premise)
./lib.rs:        goal: &str,
./lib.rs:        decode_fpa_arg_rs(&args, hyps, goal, arg_idx)
./lib.rs:        goal: &str,
./lib.rs:        match encode_fpa_arg_unbounded(&args, hyps, goal, arg) {
./lib.rs:    fn goals_to_total_distances_tensors(
./lib.rs:            Ok(goals_to_total_distances_tensors_rs(args, filename, None)
./lib.rs:    fn goals_to_total_distances_tensors_with_meta(
./lib.rs:            let (_, goals, outputs) =
./lib.rs:                goals_to_total_distances_tensors_rs(args, filename, Some(metadata))
./lib.rs:            Ok((goals, outputs))
./lib.rs:    fn goal_enc_get_num_tokens(_py: Python, metadata: &GoalEncMetadata) -> i64 {
./lib.rs:        goal_enc_get_num_tokens_rs(metadata)
./lib.rs:    fn goal_enc_tokenize_goal(
./lib.rs:        tokenize_goal(args, metadata, s)
./lib.rs:        goal: String,
./lib.rs:            &goal,
./models/evaluator_common.rs:    let close_goal =
./models/evaluator_common.rs:            close_goal(
./models/evaluator_common.rs:            close_goal(
./models/evaluator_common.rs:    close_goal(
./models/features_polyarg_predictor.rs:    goal: Vec<Token>,
./models/features_polyarg_predictor.rs:            ScrapedData::Tactic(t) => { spinner.inc(1); /*println!("ScrapedTactic datum.context.fg_goals: {:?}", t.context.fg_goals);*/Some(t)},
./models/features_polyarg_predictor.rs:                &remove_paths_from_goal(scraped.context.focused_goal()),
./models/features_polyarg_predictor.rs:                    equality_hyp_feature(hyp, &remove_paths_from_goal(scraped.context.focused_goal())),
./models/features_polyarg_predictor.rs:    let tokenized_goals: Vec<_> = raw_data
./models/features_polyarg_predictor.rs:                //tokenizer.pathstokenize(&tac.context.focused_goal()).0, TODO
./models/features_polyarg_predictor.rs:                tokenizer.tokenize(&remove_paths_from_goal(&tac.context.focused_goal())),
./models/features_polyarg_predictor.rs:    //            tokenizer.pathstokenize(&tac.context.focused_goal()).1,
./models/features_polyarg_predictor.rs:    //    }).progress_with(ProgressBar::new(length).with_message("Tokenizing goals")
./models/features_polyarg_predictor.rs:    let goal_symbols_mask = raw_data
./models/features_polyarg_predictor.rs:        .map(|scraped| get_goal_mask(&remove_paths_from_goal(scraped.context.focused_goal()), args.max_length))
./models/features_polyarg_predictor.rs:        .progress_with(ProgressBar::new(length).with_message("Getting goal masks")
./models/features_polyarg_predictor.rs:            tokenized_goals,
./models/features_polyarg_predictor.rs:            goal_symbols_mask,
./models/features_polyarg_predictor.rs:fn get_goal_mask(goal: &str, max_length: usize) -> Vec<bool> {
./models/features_polyarg_predictor.rs:    let updated_goal = remove_paths_from_goal(&goal);
./models/features_polyarg_predictor.rs:    let words = get_words(&updated_goal);
./models/features_polyarg_predictor.rs:        .map(|goal_word| STARTS_WITH_LETTER.is_match(goal_word))
./models/features_polyarg_predictor.rs:    goal: String,
./models/features_polyarg_predictor.rs:    let score = gestalt_ratio(&remove_paths_from_goal(&goal), get_hyp_type(&premise));
./models/features_polyarg_predictor.rs:    let eq_feat = equality_hyp_feature(&premise, &remove_paths_from_goal(&goal));
./models/features_polyarg_predictor.rs:                &remove_paths_from_goal(&ctxt.obligation.goal), //HERE?
./models/features_polyarg_predictor.rs:        .map(|(premises, context)| score_hyps(premises, &remove_paths_from_goal(&context.obligation.goal)))
./models/features_polyarg_predictor.rs:                    vec![*score, equality_hyp_feature(premise, &remove_paths_from_goal(&ctxt.obligation.goal))]
./models/features_polyarg_predictor.rs:    let tgoals_batch = context_batch
./models/features_polyarg_predictor.rs:                //tokenizer.pathstokenize(&ctxt.obligation.goal).0, TODO
./models/features_polyarg_predictor.rs:                tokenizer.tokenize(&remove_paths_from_goal(&ctxt.obligation.goal)),
./models/features_polyarg_predictor.rs:    //            tokenizer.pathstokenize(&ctxt.obligation.goal).1,
./models/features_polyarg_predictor.rs:    let goal_symbols_mask = context_batch
./models/features_polyarg_predictor.rs:        .map(|ctxt| get_goal_mask(&remove_paths_from_goal(&ctxt.obligation.goal), args.max_length))
./models/features_polyarg_predictor.rs:        tgoals_batch,
./models/features_polyarg_predictor.rs:        goal_symbols_mask,
./models/features_polyarg_predictor.rs:    goal: String,
./models/features_polyarg_predictor.rs:        &remove_paths_from_goal(&goal),
./models/features_polyarg_predictor.rs:    let updated_goal: String = remove_paths_from_goal(&goal);
./models/features_polyarg_predictor.rs:    let premise_scores = score_hyps(&all_premises, &updated_goal);
./models/features_polyarg_predictor.rs:        .map(|(premise, score)| vec![*score, equality_hyp_feature(premise, &updated_goal)])
./models/features_polyarg_predictor.rs:    //let (pretokenized_goals, pretokenized_paths) = tokenizer.pathstokenize(&goal); TODO
./models/features_polyarg_predictor.rs:    let pretokenized_goals = tokenizer.tokenize(&remove_paths_from_goal(&goal));
./models/features_polyarg_predictor.rs:    let tokenized_goals = normalize_sentence_length(pretokenized_goals, args.max_length, 0);
./models/features_polyarg_predictor.rs:    // let goal_symbols_mask = get_goal_mask_after_paths(&goal, args.max_length); TODO
./models/features_polyarg_predictor.rs:    let goal_symbols_mask = get_goal_mask(&updated_goal, args.max_length);
./models/features_polyarg_predictor.rs:        vec![tokenized_goals],
./models/features_polyarg_predictor.rs:        vec![goal_symbols_mask],
./models/features_polyarg_predictor.rs:    goal: &str,
./models/features_polyarg_predictor.rs:    let arg = decode_fpa_arg_rs(&args, premises, &remove_paths_from_goal(&goal), arg_idx);
./models/features_polyarg_predictor.rs:    goal: &str,
./models/features_polyarg_predictor.rs:            // assert!(tidx < get_words(goal).len(), format!("{}, {:?}, {}", goal, get_words(goal), tidx));
./models/features_polyarg_predictor.rs:            if tidx >= get_words(&remove_paths_from_goal(&goal)).len() {
./models/features_polyarg_predictor.rs:                get_words(&remove_paths_from_goal(&goal))[tidx].to_string()
./models/features_polyarg_predictor.rs:fn equality_hyp_feature(hyp: &str, goal: &str) -> f64 {
./models/features_polyarg_predictor.rs:    let normalized_goal = goal.replace("\n", " ");
./models/features_polyarg_predictor.rs:        if normalized_goal.contains(left_side.trim()) && right_side != "" {
./models/features_polyarg_predictor.rs:        } else if normalized_goal.contains(right_side.trim()) && right_side != "" {
./models/features_polyarg_predictor.rs:    goal: &str,
./models/features_polyarg_predictor.rs:        let updated_goal = remove_paths_from_goal(&goal);
./models/features_polyarg_predictor.rs:        let goal_symbols = get_words(&updated_goal);
./models/features_polyarg_predictor.rs:        match goal_symbols
./models/features_polyarg_predictor.rs:                arg, hyps, remove_paths_from_goal(&goal)
./models/features_polyarg_predictor.rs:        let focused_goal = scraped.context.focused_goal();
./models/features_polyarg_predictor.rs:        let updated_goal : String = remove_paths_from_goal(focused_goal);
./models/features_polyarg_predictor.rs:        let goal_symbols = get_words(&updated_goal);
./models/features_polyarg_predictor.rs:        //println!("{:?}", goal_symbols);
./models/features_polyarg_predictor.rs:        match goal_symbols
./models/features_polyarg_predictor.rs:            remove_paths_from_goal(scraped.context.focused_goal())
./models/features_polyarg_predictor.rs:fn remove_paths_from_goal(goal: &str) -> String {
./models/features_polyarg_predictor.rs:    println!("{}", goal);
./models/features_polyarg_predictor.rs:    let mut updated_goal: String = "".to_string();
./models/features_polyarg_predictor.rs:    let words: Vec<&str> = goal.split(" ").collect();
./models/features_polyarg_predictor.rs:        updated_goal = updated_goal + " " + split[0];
./models/features_polyarg_predictor.rs:    println!("{}", updated_goal);
./models/features_polyarg_predictor.rs:    updated_goal
./models/goal_enc_evaluator.rs:pub fn goals_to_total_distances_tensors_rs(
./models/goal_enc_evaluator.rs:    let tokenized_goals: Vec<Vec<i64>> = tactics
./models/goal_enc_evaluator.rs:        .map(|tac| truncate_to_length(tokenizer.pathstokenize(&tac.context.focused_goal()).0, args.max_length))
./models/goal_enc_evaluator.rs:        tokenized_goals,
./models/goal_enc_evaluator.rs:pub fn goal_enc_get_num_tokens_rs(metadata: &GoalEncMetadata) -> i64 {
./models/goal_enc_evaluator.rs:pub fn goal_enc_get_num_paths_tokens_rs(metadata: &GoalEncMetadata) -> i64 {
./models/goal_enc_evaluator.rs:pub fn tokenize_goal(args: DataloaderArgs, metadata: &GoalEncMetadata, goal: String) -> Vec<i64> {
./models/goal_enc_evaluator.rs:            .tokenize(&remove_paths_from_goal(&goal)),
./models/goal_enc_evaluator.rs:fn remove_paths_from_goal(goal: &str) -> String {
./models/goal_enc_evaluator.rs:    println!("{}", goal);
./models/goal_enc_evaluator.rs:    let mut updated_goal: String = "".to_string();
./models/goal_enc_evaluator.rs:    let words: Vec<&str> = goal.split(" ").collect();
./models/goal_enc_evaluator.rs:        updated_goal = updated_goal + " " + split[0];
./models/goal_enc_evaluator.rs:    println!("{}", updated_goal);
./models/goal_enc_evaluator.rs:    updated_goal
./models/mod.rs:pub mod goal_enc_evaluator;
./scraped_data.rs:    pub goal: String,
./scraped_data.rs:    fn new(hypotheses: Vec<String>, goal: String) -> Self {
./scraped_data.rs:        Obligation { hypotheses, goal }
./scraped_data.rs:    pub fg_goals: Vec<Obligation>,
./scraped_data.rs:    pub bg_goals: Vec<Obligation>,
./scraped_data.rs:    pub shelved_goals: Vec<Obligation>,
./scraped_data.rs:    pub given_up_goals: Vec<Obligation>,
./scraped_data.rs:            fg_goals: vec![],
./scraped_data.rs:            bg_goals: vec![],
./scraped_data.rs:            shelved_goals: vec![],
./scraped_data.rs:            given_up_goals: vec![],
./scraped_data.rs:    pub fn focused_goal(&self) -> &String {
./scraped_data.rs:        match self.fg_goals.first() {
./scraped_data.rs:            Some(obl) => &obl.goal,
./scraped_data.rs:        match self.fg_goals.first() {
./scraped_data.rs:                    "prev_goal": tac.context.focused_goal(),
./scraped_data.rs:            // let mut updated_goal: String = "".to_string();
./scraped_data.rs:            // let words: Vec<&str> = datum.context.focused_goal().split(" ").collect();
./scraped_data.rs:            //     updated_goal = updated_goal + " " + split[0];
./scraped_data.rs:                            // match get_binder_var(&updated_goal, var_idx) {
./scraped_data.rs:                            if datum.context.focused_goal().contains("|-path-|") {
./scraped_data.rs:                                match get_binder_var_with_paths(&datum.context.focused_goal(), var_idx) {
./scraped_data.rs:                                            "Binder var issue on token {} for goal {}", 
./scraped_data.rs:                                            datum.context.focused_goal()
./scraped_data.rs:                                match get_binder_var(&datum.context.focused_goal(), var_idx) {
./scraped_data.rs:                                            "Binder var issue on token {} for goal {}", 
./scraped_data.rs:                                            datum.context.focused_goal()
./scraped_data.rs:fn get_binder_var(goal: &str, binder_idx: i64) -> Option<&str> {
./scraped_data.rs:    let forall_match = match FORALL.find(goal) {
./scraped_data.rs:    let rest_goal = &goal[forall_match.end()..];
./scraped_data.rs:    for w in get_symbols(rest_goal) {
./scraped_data.rs:fn get_binder_var_with_paths(goal: &str, binder_idx: i64) -> Option<&str> {
./scraped_data.rs:    let forall_match = match FORALL.find(goal) {
./scraped_data.rs:    let rest_goal = &goal[forall_match.end()..];
./scraped_data.rs:    let words: Vec<&str> = rest_goal.split(" ").collect();
./tokenizer.rs:        let goalstokens: Vec<Token> = words
./tokenizer.rs:        assert_eq!(goalstokens.len(), pathstokens.len());
./tokenizer.rs:        (goalstokens, pathstokens)
